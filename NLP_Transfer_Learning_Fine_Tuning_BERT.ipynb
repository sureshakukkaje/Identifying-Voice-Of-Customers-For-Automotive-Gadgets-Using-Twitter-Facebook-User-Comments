{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_Transfer Learning for NLP - Fine-Tuning BERT for Text Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAKrL7ihGhk6"
      },
      "source": [
        "Ref: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5pidpYVIREV"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmhC1_DOGNh_",
        "outputId": "22648ee8-1cf5-43ef-fc8b-1ae52b11e890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYgoHMZeIWzO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fLgua7JU10"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTmozJsaIW4d",
        "outputId": "e2ee7cc5-0cc8-4b3f-9549-eb0b26107456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_excel('comments_data_01.xlsx')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I just bought this projector and i am clueless...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>You promised me a multi card reader for leavin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Hello Apeman, your team is not responding to m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Great picture, I am so excited about looking f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>I have just bought the Apeman 550 dashcam and ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  I just bought this projector and i am clueless...\n",
              "1      0  You promised me a multi card reader for leavin...\n",
              "2      0  Hello Apeman, your team is not responding to m...\n",
              "3      1  Great picture, I am so excited about looking f...\n",
              "4      0  I have just bought the Apeman 550 dashcam and ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cay98QmnIW7i",
        "outputId": "97bb19d3-e408-47e8-f440-1431ed0adf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9546, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNJvcgOtIW97",
        "outputId": "4018f8e5-09ae-4989-8b85-b84343b3ca24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.676618\n",
              "1    0.323382\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmCcePRfaMOT"
      },
      "source": [
        "\"0\" Negative and \n",
        "\"1\" Positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRxYUMtQKcTZ"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqxwsGHHIXDF"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8pyMV8Kqy2"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBIawLxHKmSW"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8QCjI3MKmVe"
      },
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffjIg2fnKmXe",
        "outputId": "f267ddb3-0391-4634-82f9-9fe1eb2dd9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSRxycgGLahQ"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUqyywFrKmap",
        "outputId": "3c15ef5f-5854-4121-90ca-598e46c1e1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f71b701a9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS1ElEQVR4nO3df6xk5X3f8fenYBOHjVgo7RVeUHerbBphtnHsK6ByVN2NG7NAFBypsrCQvWsTbf4AxW6RmiVWhRvXFVGN3Vh2aTdha0hc31D/iFeAQ9dbXyH/gQ3rIpYfpmzMOma1hqRg8NqWk3W//WPOimF9796Zu/fOnTvP+yWN5pznPGfm+c65+5mZZ87MpqqQJLXh7632ACRJo2PoS1JDDH1JaoihL0kNMfQlqSFnrvYATuX888+vjRs3DrXPD37wA84+++yVGdCIWMN4mIQaYDLqsIbhHDhw4G+q6h/Mt22sQ3/jxo08/PDDQ+0zNzfHzMzMygxoRKxhPExCDTAZdVjDcJJ8e6FtTu9IUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQRUM/yUVJvpLkiSSPJ3lf1/7BJEeSPNJdrurb5+Ykh5I8leSKvvZtXduhJLtWpiRJ0kIG+XLWceCmqvpGkp8DDiTZ1237WFV9pL9zkouBa4E3AK8HvpzkF7rNnwR+DXgWeCjJ3qp6YjkKkSQtbtHQr6qjwNFu+ftJngQ2nGKXa4DZqvox8EySQ8Cl3bZDVfUtgCSzXd+JC/2Nu+4dqN/hW69e4ZFI0qtlmP85K8lG4AHgEuBfAzuAl4GH6b0beDHJJ4AHq+pPu33uAL7U3cS2qvqtrv1dwGVVdeNJ97ET2AkwNTX15tnZ2aEKOnbsGOvWrRtqn+V28MhLA/XbsuGcedvHoYbTZQ3jYxLqsIbhbN269UBVTc+3beDf3kmyDvgc8P6qejnJ7cCHgOqubwPee7qDrardwG6A6enpGva3KsbhNzp2DPpK/7qZedvHoYbTZQ3jYxLqsIblM1DoJ3kNvcD/dFV9HqCqnuvb/kfAPd3qEeCivt0v7No4RbskaQQGOXsnwB3Ak1X10b72C/q6/SbwWLe8F7g2yVlJNgGbga8DDwGbk2xK8lp6H/buXZ4yJEmDGOSV/luAdwEHkzzStf0e8M4kb6Q3vXMY+G2Aqno8yd30PqA9DtxQVT8BSHIjcD9wBrCnqh5fxlokSYsY5OydrwKZZ9N9p9jnw8CH52m/71T7SZJWlt/IlaSGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDVk0dBPclGSryR5IsnjSd7XtZ+XZF+Sp7vrc7v2JPl4kkNJHk3ypr7b2t71fzrJ9pUrS5I0n0Fe6R8Hbqqqi4HLgRuSXAzsAvZX1WZgf7cOcCWwubvsBG6H3pMEcAtwGXApcMuJJwpJ0mgsGvpVdbSqvtEtfx94EtgAXAPc2XW7E3h7t3wNcFf1PAisT3IBcAWwr6peqKoXgX3AtmWtRpJ0SqmqwTsnG4EHgEuAv6qq9V17gBeran2Se4Bbq+qr3bb9wO8CM8DPVNW/79r/LfCjqvrISfexk947BKampt48Ozs7VEHHjh1j3bp1Q+2z3A4eeWmgfls2nDNv+zjUcLqsYXxMQh3WMJytW7ceqKrp+badOeiNJFkHfA54f1W93Mv5nqqqJIM/e5xCVe0GdgNMT0/XzMzMUPvPzc0x7D7Lbceuewfqd/i6mXnbx6GG02UN42MS6rCG5TPQ2TtJXkMv8D9dVZ/vmp/rpm3orp/v2o8AF/XtfmHXtlC7JGlEFn2l303d3AE8WVUf7du0F9gO3Npdf7Gv/cYks/Q+tH2pqo4muR/4D30f3r4NuHl5ylibNi7wjuCmLcdf9W7h8K1Xj2pIkibcINM7bwHeBRxM8kjX9nv0wv7uJNcD3wbe0W27D7gKOAT8EHgPQFW9kORDwENdv9+vqheWpQpJ0kAWDf3uA9kssPmt8/Qv4IYFbmsPsGeYAUqSlo/fyJWkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZNHQT7InyfNJHutr+2CSI0ke6S5X9W27OcmhJE8luaKvfVvXdijJruUvRZK0mDMH6PMp4BPAXSe1f6yqPtLfkORi4FrgDcDrgS8n+YVu8yeBXwOeBR5KsreqnjiNsY/cxl33rvYQJOm0LBr6VfVAko0D3t41wGxV/Rh4Jskh4NJu26Gq+hZAktmu75oKfUla61JVi3fqhf49VXVJt/5BYAfwMvAwcFNVvZjkE8CDVfWnXb87gC91N7Otqn6ra38XcFlV3TjPfe0EdgJMTU29eXZ2dqiCjh07xrp164baZ1AHj7y0Ird7sqnXwXM/emV9y4ZzRnK/y2klj8OoTEINMBl1WMNwtm7deqCqpufbNsj0znxuBz4EVHd9G/DeJd7Wq1TVbmA3wPT0dM3MzAy1/9zcHMPuM6gdI5reuWnLcW47+MqhOXzdzEjudzmt5HEYlUmoASajDmtYPksK/ap67sRykj8C7ulWjwAX9XW9sGvjFO2SpBFZ0imbSS7oW/1N4MSZPXuBa5OclWQTsBn4OvAQsDnJpiSvpfdh796lD1uStBSLvtJP8hlgBjg/ybPALcBMkjfSm945DPw2QFU9nuRueh/QHgduqKqfdLdzI3A/cAawp6oeX/ZqJEmnNMjZO++cp/mOU/T/MPDhedrvA+4banSSpGXlN3IlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDVk09JPsSfJ8ksf62s5Lsi/J0931uV17knw8yaEkjyZ5U98+27v+TyfZvjLlSJJOZZBX+p8Ctp3UtgvYX1Wbgf3dOsCVwObushO4HXpPEsAtwGXApcAtJ54oJEmjs2joV9UDwAsnNV8D3Nkt3wm8va/9rup5EFif5ALgCmBfVb1QVS8C+/jpJxJJ0gpb6pz+VFUd7Za/C0x1yxuA7/T1e7ZrW6hdkjRCZ57uDVRVJanlGAxAkp30poaYmppibm5uqP2PHTs29D6DumnL8RW53ZNNve7V97VS9ayklTwOozIJNcBk1GENy2epof9ckguq6mg3ffN8134EuKiv34Vd2xFg5qT2ufluuKp2A7sBpqena2ZmZr5uC5qbm2PYfQa1Y9e9K3K7J7tpy3FuO/jKoTl83cxI7nc5reRxGJVJqAEmow5rWD5Lnd7ZC5w4A2c78MW+9nd3Z/FcDrzUTQPdD7wtybndB7hv69okSSO06Cv9JJ+h9yr9/CTP0jsL51bg7iTXA98G3tF1vw+4CjgE/BB4D0BVvZDkQ8BDXb/fr6qTPxyWJK2wRUO/qt65wKa3ztO3gBsWuJ09wJ6hRidJWlZ+I1eSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDzlztAYyDjbvuXe0hSNJI+Epfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNeS0TtlMchj4PvAT4HhVTSc5D/gzYCNwGHhHVb2YJMAfAlcBPwR2VNU3Tuf+WzHMKaWHb716BUciaa1bjlf6W6vqjVU13a3vAvZX1WZgf7cOcCWwubvsBG5fhvuWJA1hJaZ3rgHu7JbvBN7e135X9TwIrE9ywQrcvyRpAamqpe+cPAO8CBTwX6tqd5LvVdX6bnuAF6tqfZJ7gFur6qvdtv3A71bVwyfd5k567wSYmpp68+zs7FBjOnbsGOvWrRtqn4NHXhqq/0qbeh0896Ol7btlwznLO5glWspxGDeTUANMRh3WMJytW7ce6Jt9eZXT/RmGX6mqI0n+IbAvyTf7N1ZVJRnqWaWqdgO7Aaanp2tmZmaoAc3NzTHsPjvG7GcYbtpynNsOLu3QHL5uZnkHs0RLOQ7jZhJqgMmowxqWz2lN71TVke76eeALwKXAcyembbrr57vuR4CL+na/sGuTJI3IkkM/ydlJfu7EMvA24DFgL7C967Yd+GK3vBd4d3ouB16qqqNLHrkkaWinM70zBXyhN23PmcB/r6q/SPIQcHeS64FvA+/o+t9H73TNQ/RO2XzPady3JGkJlhz6VfUt4Jfmaf+/wFvnaS/ghqXenyTp9PmNXElqiKEvSQ2Z6P85y/8RS5JezVf6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JasiZqz0ALa+Nu+4dqN/hW69e4ZFIGkcjf6WfZFuSp5IcSrJr1PcvSS0baegnOQP4JHAlcDHwziQXj3IMktSyUU/vXAocqqpvASSZBa4BnhjxOJo36DTQoJwuktaGUYf+BuA7fevPApf1d0iyE9jZrR5L8tSQ93E+8DdLHuEY+J01WEP+4Kea1lwN85iEGmAy6rCG4fyjhTaM3Qe5VbUb2L3U/ZM8XFXTyzikkbOG8TAJNcBk1GENy2fUH+QeAS7qW7+wa5MkjcCoQ/8hYHOSTUleC1wL7B3xGCSpWSOd3qmq40luBO4HzgD2VNXjy3w3S54aGiPWMB4moQaYjDqsYZmkqlZ7DJKkEfFnGCSpIYa+JDVkokJ/Lf7EQ5KLknwlyRNJHk/yvq79vCT7kjzdXZ+72mNdTJIzkvzvJPd065uSfK07Hn/WfXg/tpKsT/LZJN9M8mSSf7bWjkOSf9X9HT2W5DNJfmbcj0OSPUmeT/JYX9u8j3t6Pt7V8miSN63eyF+xQA3/sftbejTJF5Ks79t2c1fDU0muGOVYJyb01/BPPBwHbqqqi4HLgRu6ce8C9lfVZmB/tz7u3gc82bf+B8DHqurngReB61dlVIP7Q+AvquoXgV+iV8uaOQ5JNgC/A0xX1SX0Tpa4lvE/Dp8Ctp3UttDjfiWwubvsBG4f0RgX8yl+uoZ9wCVV9U+B/wPcDND9+74WeEO3z3/u8mskJib06fuJh6r6W+DETzyMtao6WlXf6Ja/Ty9oNtAb+51dtzuBt6/OCAeT5ELgauCPu/UAvwp8tusy1jUkOQf458AdAFX1t1X1PdbYcaB3Rt7rkpwJ/CxwlDE/DlX1APDCSc0LPe7XAHdVz4PA+iQXjGakC5uvhqr6n1V1vFt9kN73kqBXw2xV/biqngEO0cuvkZik0J/vJx42rNJYliTJRuCXga8BU1V1tNv0XWBqlYY1qP8E/Bvg/3Xrfx/4Xt8f/bgfj03AXwP/rZui+uMkZ7OGjkNVHQE+AvwVvbB/CTjA2joOJyz0uK/Vf+fvBb7ULa9qDZMU+mtaknXA54D3V9XL/duqd17t2J5bm+TXgeer6sBqj+U0nAm8Cbi9qn4Z+AEnTeWsgeNwLr1XkZuA1wNn89NTDmvOuD/ui0nyAXrTuJ9e7bHAZIX+mv2JhySvoRf4n66qz3fNz51429pdP79a4xvAW4DfSHKY3rTar9KbH1/fTTPA+B+PZ4Fnq+pr3fpn6T0JrKXj8C+AZ6rqr6vq74DP0zs2a+k4nLDQ476m/p0n2QH8OnBdvfKlqFWtYZJCf03+xEM3930H8GRVfbRv015ge7e8HfjiqMc2qKq6uaourKqN9B73/1VV1wFfAf5l123ca/gu8J0k/6Rreiu9n/xeM8eB3rTO5Ul+tvu7OlHDmjkOfRZ63PcC7+7O4rkceKlvGmisJNlGb8rzN6rqh32b9gLXJjkrySZ6H0p/fWQDq6qJuQBX0fuU/C+BD6z2eAYc86/Qe+v6KPBId7mK3pz4fuBp4MvAeas91gHrmQHu6Zb/cffHfAj4H8BZqz2+Rcb+RuDh7lj8OXDuWjsOwL8Dvgk8BvwJcNa4HwfgM/Q+g/g7eu+4rl/ocQdC7yy9vwQO0jtTaVxrOERv7v7Ev+v/0tf/A10NTwFXjnKs/gyDJDVkkqZ3JEmLMPQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ/4/szGb+9JctzgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCAV_aW3Kmcj"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88W7Jhv7LfKj",
        "outputId": "c561fbe0-37e9-4dad-99ad-caf56235cf2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaUCEqhFLxg1"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5LKiwHuLfNE"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRrZYqUjMj6p"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjxDMKS1LfQF"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzpOji7IMwx-"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTXPZaPBL2Yv"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6YXRTVM1yu"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcAeLYaqL2b3"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_yf-Q01L2g5"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvGHwmOVL2kd"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0_jA96pNVDA"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBl8gzWjL2nI",
        "outputId": "41d95ab2-74b9-436f-b5a7-cdb3084226cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.73892007 1.54637505]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVQ2HOZ8L2pZ"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxdkHWZqNdTT"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ObnJwhL2rx"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nM3ch9L2uf"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa4q5Tl5N7cW"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV5FHTj9LfSf",
        "outputId": "655b4989-6df0-41f8-f528-44b9eed34231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.702\n",
            "Validation Loss: 0.680\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.699\n",
            "Validation Loss: 0.703\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.683\n",
            "Validation Loss: 0.695\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.675\n",
            "Validation Loss: 0.663\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.670\n",
            "Validation Loss: 0.657\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.669\n",
            "Validation Loss: 0.657\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.665\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.672\n",
            "Validation Loss: 0.654\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.662\n",
            "Validation Loss: 0.651\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    239.\n",
            "  Batch   100  of    239.\n",
            "  Batch   150  of    239.\n",
            "  Batch   200  of    239.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.666\n",
            "Validation Loss: 0.654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6nL9VLGOE-s"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpfUk_gAN8in",
        "outputId": "915dd7c5-4f44-43a6-dc70-e84cf5eaf55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WH9x1NdOKYO"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQD8uqOsN8lt"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpADI4Z9N8oz",
        "outputId": "2c83a4d8-c79b-4e26-b142-b6c48ad6cc10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.69      0.73       646\n",
            "           1       0.46      0.56      0.51       309\n",
            "\n",
            "    accuracy                           0.65       955\n",
            "   macro avg       0.61      0.62      0.62       955\n",
            "weighted avg       0.67      0.65      0.66       955\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg70shSFN8rU",
        "outputId": "289ad449-ff4a-47df-e784-35beeaa4905c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>447</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>137</td>\n",
              "      <td>172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    0    1\n",
              "row_0          \n",
              "0      447  199\n",
              "1      137  172"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfJUxiLN8vV"
      },
      "source": [
        "###---------------------------------###"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}